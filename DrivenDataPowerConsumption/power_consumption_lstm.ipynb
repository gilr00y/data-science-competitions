{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,RobustScaler\n",
    "RANDOM_SEED = 2018\n",
    "seed(RANDOM_SEED)\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "data_path = '/power_consumption/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumption_full = pd.read_csv(data_path + 'consumption_train.csv', parse_dates=['timestamp'])\n",
    "consumption_full = consumption_full.drop('Unnamed: 0', axis=1)\n",
    "t = consumption_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = {}\n",
    "c_f = {}\n",
    "for ser_id, ser_df in consumption_full.groupby('series_id'):\n",
    "    consumption = ser_df.consumption.reshape(-1,1)\n",
    "#     scaler[ser_id] = MinMaxScaler() # range(0,1)\n",
    "    scaler[ser_id] = StandardScaler()\n",
    "    scaler[ser_id].fit(consumption)\n",
    "    t = scaler[ser_id].transform(consumption)\n",
    "    ser_df['consumption_norm'] = t\n",
    "    if type(c_f) == pd.DataFrame:\n",
    "        c_f = c_f.append(ser_df)\n",
    "    else:\n",
    "        c_f = ser_df\n",
    "consumption_full = c_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumption_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumption_meta = pd.read_csv(data_path + 'meta.csv')\n",
    "# consumption_meta.surface.unique()\n",
    "consumption_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = {\n",
    "    'xx-small': 0,\n",
    "    'x-small': 0.15,\n",
    "    'small': 0.3,\n",
    "    'medium': 0.5,\n",
    "    'large': 0.65,\n",
    "    'x-large': 0.8,\n",
    "    'xx-large': 1.0\n",
    "}\n",
    "on_off = {\n",
    "    0: 'monday_is_day_off',\n",
    "    1: 'tuesday_is_day_off',\n",
    "    2: 'wednesday_is_day_off',\n",
    "    3: 'thursday_is_day_off',\n",
    "    4: 'friday_is_day_off',\n",
    "    5: 'saturday_is_day_off',\n",
    "    6: 'sunday_is_day_off'\n",
    "}\n",
    "consumption_meta['area'] = consumption_meta.surface.map(lambda x: sizes[x])\n",
    "# consumption_meta.apply(lambda row: row.series_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.to_datetime(consumption_full.timestamp).map(lambda x: x.dayofweek)\n",
    "consumption_full['day_of_week'] = consumption_full.timestamp.map(lambda x: (x.dayofweek/6.0))\n",
    "consumption_full['hour'] = consumption_full.timestamp.map(lambda x: (x.hour/23.0))\n",
    "# consumption_full['area'] = consumption_full.series_id.map(lambda x: (\n",
    "#     consumption_meta[consumption_meta.series_id == x].area.values[0]\n",
    "# ))\n",
    "def is_on(row):\n",
    "    try:\n",
    "        dow = row.timestamp.dayofweek\n",
    "        off = consumption_meta[consumption_meta.series_id == row.series_id][on_off[dow]].values[0]\n",
    "    except Exception as e:\n",
    "#         print(consumption_meta[consumption_meta.series_id == row.series_id])\n",
    "        print(not consumption_meta[consumption_meta.series_id == row.series_id][on_off[dow]].values[0])\n",
    "        raise e\n",
    "    return not off\n",
    "\n",
    "consumption_full['is_on'] = consumption_full.apply(is_on, axis=1)\n",
    "\n",
    "consumption_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumption_full['n_is_on'] = consumption_full.is_on +0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(consumption_full.day_of_week.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose subset of series for training\n",
    "frac_series_to_use = 0.7\n",
    "\n",
    "rng = np.random.RandomState(seed=RANDOM_SEED)\n",
    "series_ids = c_f.series_id.unique()\n",
    "training_mask = rng.binomial(1,\n",
    "                           frac_series_to_use,\n",
    "                           size=series_ids.shape).astype(bool)\n",
    "testing_mask = ~training_mask\n",
    "training_series = series_ids[training_mask]\n",
    "testing_series = series_ids[testing_mask]\n",
    "\n",
    "# reduce training data to series subset\n",
    "consumption_train = consumption_full[consumption_full.series_id.isin(training_series)]\n",
    "consumption_test = consumption_full[consumption_full.series_id.isin(testing_series)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_is_on_idx = 0\n",
    "hour_idx = 1\n",
    "day_of_week_idx = 2\n",
    "consumption_norm_idx = 3\n",
    "def create_lagged_features(df, lag=1):\n",
    "    # Takes in DF\n",
    "    if not type(df) == pd.DataFrame:\n",
    "#         df = pd.DataFrame(df, columns=['consumption_norm'])\n",
    "        raise Exception('NOT A DATAFRAME')\n",
    "    def _rename_lag(ser, j):\n",
    "        ser.name = ser.name + '_{j}'.format(j=j)\n",
    "        return ser\n",
    "    if len(df) == 24:\n",
    "        last_row = df[-1:]\n",
    "        df = df.append(df[-1:], ignore_index=True)\n",
    "    df = df[['consumption_norm', 'day_of_week', 'hour', 'n_is_on']]\n",
    "    for i in range(1, lag+1):\n",
    "        df = df.join(df[['consumption_norm', 'day_of_week', 'hour', 'n_is_on']].shift(i), rsuffix=i)#.pipe(_rename_lag, i))\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "create_lagged_features(consumption_full[consumption_full.series_id==100003], lag=24).head()\n",
    "#     [cold_start_test.series_id==100090].consumption, lag=24\n",
    "# ).head()[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# DEFINE THE TF MODEL\n",
    "# * FCNN with 2 hidden layers\n",
    "# Define input/output vars\n",
    "# Include rolling average as input(weekly, monthly)\n",
    "input_num_units = 24\n",
    "output_num_units = 4\n",
    "h1_units = 48\n",
    "h2_units = 28\n",
    "metad_size = 1\n",
    "num_neurons = 64\n",
    "lstm_sizes = [num_neurons, num_neurons/2, num_neurons/2, num_neurons/2, num_neurons]#np.ceil(0.5 * num_neurons), np.ceil(0.5 * num_neurons)]\n",
    "# dropout_keep_prob = 0.9\n",
    "dropout_keep_prob=1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units, output_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "# observed_mean = tf.placeholder(tf.float32, [None, 1])\n",
    "# Define hyperparams\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "weights = {\n",
    "#     'h1': tf.Variable(tf.random_normal([int(input_num_units * lstm_sizes[-1] + metad_size), h1_units])),\n",
    "    'h1': tf.Variable(tf.random_normal([int(lstm_sizes[-1]), h1_units])),\n",
    "#     'h2': tf.Variable(tf.random_normal([h1_units, h2_units])),\n",
    "    'output': tf.Variable(tf.random_normal([h1_units, output_num_units]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([h1_units])),\n",
    "#     'h2': tf.Variable(tf.random_normal([h2_units])),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units]))\n",
    "}\n",
    "\n",
    "lstms = [tf.nn.rnn_cell.BasicLSTMCell(num_units=size, activation=tf.nn.elu)\n",
    "         for size in lstm_sizes]\n",
    "drops = [tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=dropout_keep_prob) for lstm in lstms]\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(drops)\n",
    "\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state)\n",
    "# lstm_outputs: (?, 24, 30) or (len(X), num_inputs, num_neurons in last lstm layer)\n",
    "\n",
    "# Select only last output from LSTM to feed to FC \n",
    "# trunc_lstm_out = tf.reshape(lstm_outputs, [-1, input_num_units * lstm_sizes[-1]] )\n",
    "trunc_lstm_out = lstm_outputs[:,-1]\n",
    "# trunc_lstm_out: (?, 30)\n",
    "\n",
    "# * Combine stacked_lstm_outputs with series metadata to pass to FC layersii\n",
    "# aug_stacked = tf.concat([trunc_lstm_out, observed_mean], 1)\n",
    "# h1_layer = tf.add(tf.matmul(aug_stacked, weights['h1']), biases['h1'])\n",
    "h1_layer = tf.add(tf.matmul(trunc_lstm_out, weights['h1']), biases['h1'])\n",
    "h1_layer = tf.nn.relu(h1_layer)\n",
    "\n",
    "# h2_layer = tf.add(tf.matmul(h1_layer, weights['h2']), biases['h2'])\n",
    "# h2_layer = tf.nn.relu(h2_layer)\n",
    "\n",
    "output_layer = tf.add(tf.matmul(h1_layer, weights['output']), biases['output'])\n",
    "\n",
    "cost = tf.reduce_mean(tf.abs(output_layer - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_t = []\n",
    "test_len = 1000 # Number of series to test\n",
    "lag = 24\n",
    "epochs = 1\n",
    "checkpoint = './pc_v5_n_is_on.ckpt'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "#     saver.restore(sess, checkpoint)\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch: \" + str(i))\n",
    "        ctr = 0\n",
    "        for ser_id, ser_df in consumption_train.groupby('series_id'):\n",
    "            print('#' + str(ctr) + ': ' + str(ser_id))\n",
    "            ctr += 1\n",
    "            if ctr == test_len:\n",
    "                print('REACHED TEST LENGTH, STOPPING')\n",
    "                break\n",
    "            state = sess.run(init_state) # Re-initializes empty state\n",
    "            train_lagged = create_lagged_features(ser_df, lag=lag).values\n",
    "            train_lagged = np.flip(train_lagged, axis=1)\n",
    "            train_scaled_in = train_lagged[:,:-output_num_units].reshape(-1, lag, output_num_units)\n",
    "            train_scaled_out = train_lagged[:,-output_num_units:].reshape(-1, output_num_units)\n",
    "            for j in range(int(np.floor(train_scaled_in.shape[0]/batch_size))):\n",
    "#                 state, out = sess.run([final_state, output_layer], feed_dict={\n",
    "                state, out = sess.run([final_state, optimizer], feed_dict={\n",
    "                    x: train_scaled_in[batch_size*j:batch_size*j+batch_size],\n",
    "                    y: train_scaled_out[batch_size*j:batch_size*j+batch_size],\n",
    "                    init_state: state\n",
    "                })\n",
    "\n",
    "                if j%100 == 0:\n",
    "                    h, c = sess.run([output_layer, cost], feed_dict={\n",
    "                        x: train_scaled_in[batch_size*j:batch_size*j+batch_size],\n",
    "                        y: train_scaled_out[batch_size*j:batch_size*j+batch_size],\n",
    "                        init_state: state\n",
    "                    })\n",
    "                    print(\"Cost @ \" + str(j) + ': ' + str(c))\n",
    "                    print('Prediction: ' + str(h))\n",
    "                    print('Actual: ' + str(train_scaled_out[batch_size*j:batch_size*j+batch_size]))\n",
    "#                           str(h[0][0]\n",
    "#                               - train_scaled_out[batch_size*j:batch_size*j+batch_size][0][0]))\n",
    "            saver.save(sess, checkpoint)\n",
    "#         c_t.append(sess.run(cost, feed_dict={\n",
    "#             x: train_scaled_in,\n",
    "#             y: train_scaled_out\n",
    "#         }))\n",
    "#         print('Epoch :',i,'Cost :',c_t[i])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cold_start_len = 24 * 2\n",
    "consumption_test['prediction'] = np.nan\n",
    "consumption_test['error'] = np.nan\n",
    "\n",
    "# Returns error score for series_id\n",
    "def run_test(series_id, df_return=False):\n",
    "    with tf.Session() as local_sess:\n",
    "        saver_i = tf.train.Saver()\n",
    "        cold_start_len = 24 * 2\n",
    "\n",
    "        saver_i.restore(local_sess, checkpoint) # Start each series from scratch to avoid test-order effects\n",
    "        test_cold_start = (consumption_test\n",
    "                           [consumption_test.series_id == series_id].dropna(subset=['consumption_norm'])\n",
    "                           [:cold_start_len])\n",
    "        test_pred = consumption_test[consumption_test.series_id == series_id][cold_start_len:]\n",
    "        cold_input = np.flip(create_lagged_features(\n",
    "            test_cold_start,\n",
    "            lag=24\n",
    "        ).values, axis=1)\n",
    "        state = local_sess.run(init_state) # Zero-state\n",
    "        for cold_in in cold_input:\n",
    "            c_in = cold_in[:-1*output_num_units].reshape(-1, 24, output_num_units)\n",
    "            cold_out = [cold_in[-1*output_num_units:]]\n",
    "            h, state = local_sess.run([optimizer, final_state], feed_dict={\n",
    "                x: c_in,\n",
    "                y: cold_out,\n",
    "                init_state: state\n",
    "            })\n",
    "        X = c_in\n",
    "        X[0] = np.roll(X[0], -1, axis=0)\n",
    "        X[0][-1] = cold_out[0]\n",
    "        last_state = state\n",
    "        count = 0\n",
    "        for idx, row in test_pred.iterrows():\n",
    "            last_last_state = last_state\n",
    "            last_state = state\n",
    "            if count > 0:\n",
    "    #             print(count, test_pred, h);raise\n",
    "                X[0][-1][day_of_week_idx] = row.day_of_week\n",
    "                X[0][-1][n_is_on_idx] = row.n_is_on\n",
    "                X[0][-1][hour_idx] = row.hour\n",
    "            count += 1\n",
    "\n",
    "            h, state = local_sess.run([output_layer, final_state], feed_dict={\n",
    "                x: X,\n",
    "                init_state: state\n",
    "            })\n",
    "            prediction = scaler[series_id].inverse_transform(h[:, -1].reshape(-1, 1))\n",
    "#             if prediction < 0:\n",
    "#                 h[0][consumption_norm_idx] = -1\n",
    "#             h[0][consumption_norm_idx] = -0.5\n",
    "            prediction = scaler[series_id].inverse_transform(h[:, -1].reshape(-1, 1))\n",
    "            test_pred.at[idx,'prediction'] = prediction\n",
    "            test_pred.at[idx, 'error'] = abs(row.consumption - prediction)\n",
    "            # Push all values to the left\n",
    "            X = np.array([np.roll(X[0], -1, axis=0)])\n",
    "            # Replace the (formerly last) value with latest prediction.\n",
    "            X[0][-1] = h\n",
    "        err = test_pred.error.mean()/test_pred.consumption.mean()\n",
    "        if df_return:\n",
    "            return (err, test_pred)\n",
    "        else:\n",
    "            return err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for series_id in consumption_test[:int(np.ceil(len(consumption_test)))].series_id.unique():\n",
    "    errors.append((series_id, run_test(series_id, df_return=False)))\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_df = pd.DataFrame(data=errors, columns=['series_id', 'error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# err_df.sort_values('error', ascending=False).head()\n",
    "err_df.sort_values('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ser_id in err_df.sort_values('error').series_id[-5:]:\n",
    "    print(ser_id)\n",
    "    res = run_test(ser_id, df_return=True)\n",
    "    print(ser_id, res[0])\n",
    "    res[1].plot(\n",
    "        x='timestamp', y=['consumption', 'prediction'],\n",
    "        figsize=(20,5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = consumption_test\n",
    "ct[ct.prediction > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cold_start_test = pd.read_csv(data_path + 'cold_start_test.csv',\n",
    "                              parse_dates=['timestamp'], \n",
    "                              index_col=0)\n",
    "cold_start_test['hour'] = cold_start_test.timestamp.map(lambda x: (x.hour/23.0))\n",
    "cold_start_test['day_of_week'] = cold_start_test.timestamp.map(lambda x: (x.dayofweek/6.0))\n",
    "cold_start_test['is_on'] = cold_start_test.apply(is_on, axis=1)\n",
    "cold_scaler = {}\n",
    "cold_df = {}\n",
    "for ser_id, ser_df in cold_start_test.groupby('series_id'):\n",
    "    consumption = ser_df.consumption.reshape(-1,1)\n",
    "#     cold_scaler[ser_id] = MinMaxScaler() # range(0,1)\n",
    "    cold_scaler[ser_id] = StandardScaler()\n",
    "    cold_scaler[ser_id].fit(consumption)\n",
    "    t = cold_scaler[ser_id].transform(consumption)\n",
    "    ser_df['consumption_norm'] = t\n",
    "    if type(cold_df) == pd.DataFrame:\n",
    "        cold_df = cold_df.append(ser_df)\n",
    "    else:\n",
    "        cold_df = ser_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cold_start_test['n_is_on'] = cold_start_test.is_on +0.0\n",
    "cold_df['n_is_on'] = cold_df.is_on + 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cold_start_test.n_is_on.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(data_path + 'submission_format.csv',\n",
    "                               index_col = 'pred_id',\n",
    "                               parse_dates=['timestamp'])\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_submission = submission_format.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_i.close()\n",
    "s_i = tf.InteractiveSession()\n",
    "saver_i = tf.train.Saver()\n",
    "# saver_i.restore(s_i, './pc_v5.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "on_off = {\n",
    "    0: 'monday_is_day_off',\n",
    "    1: 'tuesday_is_day_off',\n",
    "    2: 'wednesday_is_day_off',\n",
    "    3: 'thursday_is_day_off',\n",
    "    4: 'friday_is_day_off',\n",
    "    5: 'saturday_is_day_off',\n",
    "    6: 'sunday_is_day_off'\n",
    "}\n",
    "def next_vals(h, ser_id):\n",
    "    this_hour = h[hour_idx]\n",
    "    this_day = h[day_of_week_idx]\n",
    "    this_is_on = h[n_is_on_idx]\n",
    "    if round(this_hour * 23) == 23.0:\n",
    "        next_hour = 0.0\n",
    "        if round(this_day * 6) == 6.0:\n",
    "            next_day = 0\n",
    "        else:\n",
    "            next_day = this_day + 1.0/6.0\n",
    "    else:\n",
    "        next_hour = this_hour + 1.0/23.0\n",
    "        next_day = this_day\n",
    "    next_is_on = (not consumption_meta[consumption_meta.series_id == ser_id][on_off[int(round(next_day * 6))]].values[0]) + 0.0\n",
    "    return (next_hour, next_day, next_is_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lag=24\n",
    "pred_window_to_num_preds = {'hourly': 24, 'daily': 7, 'weekly': 2}\n",
    "pred_window_to_num_pred_hours = {'hourly': 24, 'daily': 7 * 24, 'weekly': 2 * 7 * 24}\n",
    "\n",
    "num_test_series = my_submission.series_id.nunique()\n",
    "cnt = 0\n",
    "for ser_id, pred_df in my_submission.groupby('series_id'):\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break\n",
    "    # Restore training \n",
    "    saver_i.restore(s_i, checkpoint)\n",
    "    learning_rate = 0.001\n",
    "    print('Starting ' + str(ser_id))\n",
    "#     cold_df[cold_df.series_id == ser_id].consumption_norm.plot()\n",
    "\n",
    "    state = s_i.run(init_state)\n",
    "    # get info about this series' prediction window\n",
    "    pred_window = pred_df.prediction_window.unique()[0]\n",
    "    num_preds = pred_window_to_num_preds[pred_window]\n",
    "    num_pred_hours = pred_window_to_num_pred_hours[pred_window]\n",
    "    # prepare cold start data\n",
    "    series_data = cold_df[cold_df.series_id == ser_id]\n",
    "    cold_lagged = create_lagged_features(series_data, lag=lag).values\n",
    "    cold_lagged = np.flip(cold_lagged, axis=1)\n",
    "    cold_lagged_in = cold_lagged[:,:-1*output_num_units]\n",
    "    cold_lagged_out = cold_lagged[:,-1*output_num_units:].reshape(-1, output_num_units)\n",
    "#     print(\"IN:\", cold_lagged_in)\n",
    "#     print(\"OUT:\", cold_lagged_out);raise    \n",
    "    # fine tune our lstm model to this site using cold start data\n",
    "    for i, one_in in enumerate(cold_lagged_in):\n",
    "        x_in = one_in.reshape(-1, 24, output_num_units)\n",
    "#         print(\"IN\", x_in.shape)\n",
    "#         print(\"OUT\", cold_lagged_out[i:i+1]);raise\n",
    "        state, o = s_i.run([final_state, optimizer], feed_dict={\n",
    "            x: x_in,\n",
    "            y: cold_lagged_out[i:i+1],\n",
    "            init_state: state\n",
    "        })\n",
    "    \n",
    "    # Seed X for prediction with last input of warm-up\n",
    "    running_vals = np.roll(cold_lagged_in[-1].reshape(1, 24, output_num_units), -1, axis=1)\n",
    "    running_vals[0][-1] = cold_lagged_out[-1]\n",
    "    preds = np.array([])\n",
    "    for j in range(num_pred_hours):\n",
    "        pred, state = s_i.run([output_layer, final_state], feed_dict={\n",
    "            x: running_vals,\n",
    "            init_state: state\n",
    "        })\n",
    "#         print(running_vals[0][-2])\n",
    "#         print(running_vals[0][-2])\n",
    "#         print(next_vals(running_vals[0][-1], ser_id));raise\n",
    "        next_hour, next_dow, next_n_is_on = next_vals(running_vals[0][-1], ser_id)\n",
    "        running_vals = np.roll(running_vals, -1, axis=1)\n",
    "        running_vals[0][-1] = pred[0]\n",
    "        running_vals[0][-1][n_is_on_idx] = next_n_is_on\n",
    "        running_vals[0][-1][hour_idx] = next_hour\n",
    "        running_vals[0][-1][day_of_week_idx] = next_dow\n",
    "        t_pred = cold_scaler[ser_id].inverse_transform([pred[0,-1]])\n",
    "\n",
    "        if t_pred < 0 or t_pred > 1e8:\n",
    "            # If prediction is out of bounds (0,1e8), use the average\n",
    "            running_vals[0][-1][-1] = 0\n",
    "            t_pred = cold_scaler[ser_id].inverse_transform([0.0])\n",
    "\n",
    "        preds = np.append(preds, t_pred)\n",
    "    # reduce by taking sum over each sub window in pred window\n",
    "    reduced_preds = [pred.sum() for pred in np.split(preds, num_preds)]\n",
    "#     ======DEBUG PLOT======\n",
    "    series_data.consumption.append(pd.Series(preds)).reset_index().plot(figsize=(30,10))\n",
    "#     ======++++++++++======\n",
    "    # store result in submission DataFrame\n",
    "    ser_id_mask = my_submission.series_id == ser_id\n",
    "    my_submission.loc[ser_id_mask, 'consumption'] = reduced_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"my_submmission.csv\", index_label='pred_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blah = sub.fillna(value=0).groupby('series_id').max()\n",
    "len(blah[blah.temperature==0])/len(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sub2 = sub.fillna(value=0)[(sub.consumption.isnull())]\n",
    "sub2 = sub.fillna(value=sub[sub.prediction_window == 'weekly'].consumption.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub2.to_csv('sub2_filled_nas.csv', index_label='pred_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
